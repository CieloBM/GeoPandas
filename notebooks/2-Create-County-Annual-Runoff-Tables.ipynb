{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```This notebook provides a \"deep dive\" into GeoPandas and other Python packages. This is a somewhat simplified version of code I wrote for the USGS to [eventually] create national water accounting tables. The full repository for this work is here:``` https://github.com/johnpfay/USWaterAccounting\n",
    "\n",
    "```This also introduces the netCDF format - a popular format for storing and distributing multi-demensional data.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import water supply data & create supply table\n",
    "Here we download the raw supply data for a given years from from downscaled CMIP5 hydrology projections ([link](http://gdo-dcp.ucllnl.org/downscaled_cmip_projections/techmemo/BCSD5HydrologyMemo.pdf)).\n",
    "\n",
    "These data include monthly estimates of runoff at a 1/8th degree spatial resolution across the US for the period of 1950 to 2099. Estimates are provided for 21 different climate projection ensembles applied to the Variable Infiltration Capacity (VIC) Macroscale Hydrologic Model ([link](http://vic.readthedocs.io/en/master/)); see the PDF document for a complete list. For demonstration purposes, this project uses the National Center for Atmospheric Research CCSM4 2.6 projection ensembles as the base data for water supply figures. \n",
    " \n",
    "The steps involved include:\n",
    "\n",
    "* Download monthly runoff (total_runoff), in NetCDF format, from a central data repository ([ftp://gdo-dcp.ucllnl.org/pub/dcp/archive/cmip5/hydro/BCSD_mon_VIC_nc/ccsm4_rcp26_r1i1p1/](ftp://gdo-dcp.ucllnl.org/pub/dcp/archive/cmip5/hydro/BCSD_mon_VIC_nc/ccsm4_rcp26_r1i1p1/)) for a given sample year (e.g. 2000).\n",
    "\n",
    "\n",
    "* For the year:\n",
    "\n",
    "    * Extract the monthly runoff data from the downloaded NetCDF files into a 4-dimensional NumPy array (time, parameter value, latitude, longitude).\n",
    "\n",
    "    * Collapse the time dimension (months) into annual sums, resulting in a 3-dimensional array for each parameter, i.e. a single annual value for each 1/8th degree coordinate pair: rows = latitudes, columns = longitudes.\n",
    "\n",
    "    * Re-lable columns as longitude values and insert a column of latitude values. Then melt the table into a listing of lat, long, and value. \n",
    "\n",
    "    * Spatially join state FIPS codes to the data frame, using a county shapefile stored in the data folder.\n",
    "    \n",
    "\n",
    "* Summarize supply values on FIPS to create a table that can be joined to other county level data:\n",
    "\n",
    "| YEAR | FIPS | Runoff |  \n",
    "| :---: | :---: | :---: | \n",
    "| 2000 | 01001 | 0 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import sys, os, glob, time, datetime, urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame, read_file\n",
    "from geopandas.tools import sjoin\n",
    "from shapely.geometry import Point, mapping, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set script runtime variables\n",
    "The requiered inputs include the year to process, and the location of the county feature class (used to sample and summarize the 1/8th degree data to the county level). The script will write out values to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Year to run\n",
    "year = 2000\n",
    "\n",
    "#Set filename locations\n",
    "countyFN = '../Data/cb_2016_us_county_5m.shp'\n",
    "\n",
    "#Output file locations\n",
    "tidyFN = '../scratch/SupplyTableTidy.csv'\n",
    "outputFN = '../scratch/CountySupplyTable.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in the runoff data as a NetCDF4 file from the UCLLNC servers\n",
    "\n",
    "The code blocks below pulls the runoff data from the CMIP5 data ftp server as individual NetCDF4 (nc) files. Each nc file stores monthly values (n=12) across a 1/8th degree geographic grid (463 x 222). Monthly values are summed to create a data frame ('dfParam') where columns represent longitude, rows represent latitude, and the value is the runoff in mm/year. This data frame, in turn, is melted to generate a new data frame (`df`) listing lat/long pairs and the parameter value associated at that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get URLs for NCAR 2.6 scenario ensembles for runoff(ro)\n",
    "baseURL = 'ftp://gdo-dcp.ucllnl.org/pub/dcp/archive/cmip5/hydro/BCSD_mon_VIC_nc/ccsm4_rcp26_r1i1p1/'\n",
    "roURL = baseURL + \"conus_c5.ccsm4_rcp26_r1i1p1.monthly.total_runoff.{}.nc\".format(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lines fix an issue with slow network connections\n",
    "import socket\n",
    "socket.setdefaulttimeout(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retrieve the data file from the ftp server to a local (netCDF4) file\n",
    "if not os.path.exists('tmpData.nc'):\n",
    "    print \"->Downloading runoff data for year {}\".format(year)\n",
    "    urllib.urlretrieve(roURL,\"tmpData.nc\");\n",
    "else:\n",
    "    print \"->Downloading runoff data for year {}\".format(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert to netCDF object\n",
    "nc = netCDF4.Dataset(\"tmpData.nc\",mode='r')\n",
    "type(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick overview of NetCDF4 datasets\n",
    "\n",
    "The few code blocks below are a quick examination of the netCDF file format.<br>\n",
    "More info: http://www.ceda.ac.uk/static/media/uploads/ncas-reading-2015/10_read_netcdf_python.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A quick view the the netCDF object from its metadata\n",
    "print \"Description:\",nc.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The dimensions of the file\n",
    "print nc.dimensions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A look at the `time` dimension reveals 12 levels (e.g. one per month)\n",
    "print nc.dimensions['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How many levels in the latitude and longitude dimensions?\n",
    "print nc.dimensions['latitude']\n",
    "print nc.dimensions['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examining the shape of the total_runoff variable reveals it has values  \n",
    "# for the time dimension (n=12), latitude (n=222), and longitude (n=462)\n",
    "print nc.variables['total_runoff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the runoff values from the netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract the total_runoff values into a netCDF4 Variable object\n",
    "param_vals = nc.variables['total_runoff']\n",
    "type(param_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The param_vals object is a netCDF4 array with some properties\n",
    "print \"Dimensions: \",param_vals.dimensions\n",
    "print \"Units: \",param_vals.units\n",
    "print \"Shape: \",param_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If we want to get all the data for one time value, e.g. January, we use slicing.\n",
    "januaryRunoff = param_vals[0,:,:] #<-- returns a numpy array\n",
    "januaryRunoff.mean() # <-- computes the mean of all locations for January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Or, compute the time series for a specific lat-lng pair\n",
    "param_vals[:,100,120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the annual sum for each lat/lng pair\n",
    "We can sum all of the monthly runoff values using NumPy, specifying the axis we want to sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum the values in axis '0' (time), which reduces the dimension of our array\n",
    "annSum = param_vals[:,:,:].sum(axis=0)\n",
    "annSum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that it's two dimensions, we can convert it into a dataFrame\n",
    "dfParam = pd.DataFrame(annSum)\n",
    "dfParam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can again retrieve a runoff value for a specific lat-lng pair\n",
    "# This is the sum of the value computed above...\n",
    "dfParam.iloc[100,120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose values into a 'tidy' format\n",
    "We now have the runoff data in a 2-dimensional dataFrame, but this dataFrame is not 'tidy': The columns represent sequential longitudes and the rows sequential latitudes. What we need to do next is to tidy the data; more specifically, we need a table that lists each runoff value and the actual latitude and longitude in which it was recorded. And ultimately, we want to add another column which lists the county in which this coordinate point falls -- so that we can eventually compute the total runoff by county. \n",
    "\n",
    "The first step in this is to extract the actual latitude and longitude values corresponding to the rows and columns of the `dfParam` dataFrame. These values are stored in another set of variables in our original netCDF file, specifically, the latutude and longitude variables.\n",
    "\n",
    "Below we extract these values into their own respective dataFrames.(And after that, we're done with extracting values from our netCDF file, so we can clean up some things...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrLats = nc.variables['latitude'][:]\n",
    "arrLons = nc.variables['longitude'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create latitude and longitude arrays from those dimensions\n",
    "dfLats = pd.DataFrame(nc.variables['latitude'][:])\n",
    "dfLons = pd.DataFrame(nc.variables['longitude'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean up, now that we are done with the NetCDF file\n",
    "\n",
    "#Close the nc object\n",
    "nc.close()\n",
    "#Delete the nc file\n",
    "os.remove(\"tmpData.nc\")\n",
    "#Update\n",
    "urllib.urlcleanup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas `melt` function below \"un-pivots\" our `dfParam` dataFrame from rows of latitud and columns of longitude into a three column table of lat,lon,value for the current parameter (e.g. runoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rename the columns to the actual longitude values\n",
    "dfParam.columns = arrLons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a column containing the actual longitude values\n",
    "dfParam['LAT'] = arrLats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take a quick view at the results\n",
    "dfParam.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Untranspose, or \"melt\" the data so that all the longitude column names become rows, \n",
    "#  keeping the latitude values for each and assigning the values to a column named 'total_runoff'\n",
    "dfTidy = pd.melt(dfParam,id_vars=['LAT'],var_name='LON',value_name='total_runoff')\n",
    "dfTidy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the county value of each row\n",
    "Here we use Shapely and GeoPandas to:\n",
    "* Create a point feature from the lat/lng pair in each row\n",
    "* Intersect those features with the counties feature class, extracting the FIPS code into a new column in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add a geometry field to the data frame, setting the value as a new Shapely point object\n",
    "dfTidy['geometry'] = dfTidy.apply(lambda z: Point(z.LON, z.LAT), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Show one geometry point (the first); it appears as a graphic!\n",
    "dfTidy['geometry'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Show the first 10 geometry points; they appear \n",
    "dfTidy['geometry'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the our point object stored in our pandas data frame, we can now convert it into a geopandas dataframe which enables some spatial analysis functionality to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a *geopandas* dataframe from the dataframe created above\n",
    "gdfPoints = gpd.GeoDataFrame(dfTidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a second geopandas dataframe, this one from the counties file\n",
    "gdfPolygons = gpd.GeoDataFrame.from_file(countyFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the coordinate system of the points equal to the polygons\n",
    "gdfPoints.crs = gdfPolygons.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execute the spatial join of polygon attributes to the point objects\n",
    "dfMerged=sjoin(gdfPoints, gdfPolygons, how='left', op='within')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now a bit of cleaning up\n",
    "We'll drop some columns, any rows with no runoff data, and then rename some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop extranneous columns\n",
    "dfMerged.drop(['geometry','index_right','AFFGEOID','COUNTYFP','COUNTYNS','LSAD'],\n",
    "              axis=1,\n",
    "              inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop rows with no data (usually falling outside the US)\n",
    "dfMerged.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rename FIPS columns & write to a file\n",
    "dfMerged.rename(columns={'GEOID':'FIPS','STATEFP':'STATEFIPS'},inplace=True)\n",
    "dfMerged.to_csv(tidyFN,index=False,encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfMerged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute county runoff totals\n",
    "With the County FIPS column added to our tidy dataset, we can compute the total runoff for each county by grouping on the FIPS column and computing the sum of the total_runoff field. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specify the aggregate functions\n",
    "aggFunctions = {'STATEFIPS':'first',\n",
    "                'ALAND':'first',\n",
    "                'AWATER':'first',\n",
    "                'total_runoff':'sum'\n",
    "               }\n",
    "dfCounty = dfMerged.groupby('FIPS').agg(aggFunctions)\n",
    "dfCounty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write to a file\n",
    "dfCounty.to_csv(outputFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df = pd.DataFrame(dfMerged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.boxplot(by='STATEFIPS',\n",
    "           column='total_runoff',\n",
    "           figsize=(12,6)\n",
    "          );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNC = df[df.STATEFIPS == '37']\n",
    "dfNC.boxplot(by='FIPS',column='total_runoff',figsize=(12,6));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
